import { NextRequest, NextResponse } from "next/server";

// Crucible Text Generation API
// POST /api/v1/crucible/text

interface TextGenerationRequest {
  model: string;
  prompt: string;
  max_tokens?: number;
  temperature?: number;
  top_p?: number;
  stream?: boolean;
  stop?: string[];
}

interface TextGenerationResponse {
  id: string;
  object: "text_completion";
  created: number;
  model: string;
  choices: {
    text: string;
    index: number;
    finish_reason: "stop" | "length" | "content_filter";
  }[];
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  cost: {
    pyrx_amount: number;
  };
}

const SUPPORTED_MODELS = [
  "llama-3-70b",
  "llama-3-8b",
  "mistral-7b",
  "mixtral-8x7b",
  "codellama-34b",
];

const MODEL_COSTS: Record<string, number> = {
  "llama-3-70b": 0.0005,
  "llama-3-8b": 0.0001,
  "mistral-7b": 0.00008,
  "mixtral-8x7b": 0.0003,
  "codellama-34b": 0.00025,
};

export async function POST(request: NextRequest) {
  try {
    // Check authorization
    const authHeader = request.headers.get("Authorization");
    if (!authHeader?.startsWith("Bearer ")) {
      return NextResponse.json(
        { error: { message: "Missing or invalid Authorization header", type: "invalid_request_error" } },
        { status: 401 }
      );
    }

    const apiKey = authHeader.slice(7);
    if (!apiKey.startsWith("pyrax_")) {
      return NextResponse.json(
        { error: { message: "Invalid API key format", type: "invalid_request_error" } },
        { status: 401 }
      );
    }

    // Parse request body
    const body: TextGenerationRequest = await request.json();

    // Validate required fields
    if (!body.model) {
      return NextResponse.json(
        { error: { message: "model is required", type: "invalid_request_error" } },
        { status: 400 }
      );
    }

    if (!body.prompt) {
      return NextResponse.json(
        { error: { message: "prompt is required", type: "invalid_request_error" } },
        { status: 400 }
      );
    }

    // Validate model
    if (!SUPPORTED_MODELS.includes(body.model)) {
      return NextResponse.json(
        { 
          error: { 
            message: `Model '${body.model}' not supported. Available models: ${SUPPORTED_MODELS.join(", ")}`, 
            type: "invalid_request_error" 
          } 
        },
        { status: 400 }
      );
    }

    // Set defaults
    const maxTokens = body.max_tokens || 100;
    const temperature = body.temperature ?? 0.7;
    const topP = body.top_p ?? 1.0;

    // In production, this would:
    // 1. Check user's PYRX balance
    // 2. Submit job to Crucible network
    // 3. Wait for GPU worker to process
    // 4. Return results

    // For demo, return mock response
    const promptTokens = Math.ceil(body.prompt.length / 4);
    const completionTokens = Math.min(maxTokens, 50); // Demo generates ~50 tokens
    const totalTokens = promptTokens + completionTokens;
    const cost = totalTokens * (MODEL_COSTS[body.model] || 0.0001);

    const response: TextGenerationResponse = {
      id: `gen-${Date.now()}-${Math.random().toString(36).substring(2, 8)}`,
      object: "text_completion",
      created: Math.floor(Date.now() / 1000),
      model: body.model,
      choices: [
        {
          text: `[Demo response from ${body.model}] This is a placeholder response. In production, this would be generated by a GPU worker on the PYRAX network processing your prompt: "${body.prompt.substring(0, 50)}..."`,
          index: 0,
          finish_reason: "stop",
        },
      ],
      usage: {
        prompt_tokens: promptTokens,
        completion_tokens: completionTokens,
        total_tokens: totalTokens,
      },
      cost: {
        pyrx_amount: cost,
      },
    };

    return NextResponse.json(response);
  } catch (error) {
    console.error("Crucible text generation error:", error);
    return NextResponse.json(
      { error: { message: "Internal server error", type: "api_error" } },
      { status: 500 }
    );
  }
}

export async function GET() {
  return NextResponse.json({
    endpoint: "/api/v1/crucible/text",
    method: "POST",
    description: "Generate text using LLM models on the Crucible network",
    supported_models: SUPPORTED_MODELS,
    parameters: {
      model: { type: "string", required: true, description: "Model ID to use" },
      prompt: { type: "string", required: true, description: "Input prompt" },
      max_tokens: { type: "integer", required: false, default: 100, description: "Maximum tokens to generate" },
      temperature: { type: "number", required: false, default: 0.7, description: "Sampling temperature (0-2)" },
      top_p: { type: "number", required: false, default: 1.0, description: "Nucleus sampling probability" },
      stream: { type: "boolean", required: false, default: false, description: "Stream response tokens" },
      stop: { type: "array", required: false, description: "Stop sequences" },
    },
    pricing: MODEL_COSTS,
  });
}
